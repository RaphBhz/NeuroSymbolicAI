{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaphBhz/DAT615/blob/main/Assignment1_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsj45r3px7PC"
      },
      "source": [
        "# Assignment 1 - Reinforcement Learning\n",
        "\n",
        "In this assignment, we are going to first solve a simple RL exercise by reasoning about a 4-states simple MDP (Markov Decision Process), and afterwards implement a tabular RL agent that acts and (hopefully!) learns how to act in two different environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6lFMCtxx7PD"
      },
      "source": [
        "## Part 1 (3 points)\n",
        "\n",
        "Suppose you have an MDP with 4 states, numbered from 1 to 4. The possible transitions are: from state 1 to states 2 and 4, from 2 to 1 and 3, from 3 to 1 and 4. Each transition gets you a reward of -1, with 4 being the terminal state.\n",
        "\n",
        "Referring to the course slides, please answer to the questions in the markdown cells below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3UXG5Imx7PE"
      },
      "source": [
        "1) A: Visualize the MDP (maybe write it down with pen and paper) and come up with the optimal policy (the best actions for each state).\n",
        "\n",
        "Answer: In this particular context, every state transition cost us a reduction of the reward by 1. In order to maximize the reward, we need to reach the terminal state in minimum transitions.\n",
        "States 1 and 3 have direct access to state 4. It means that they can reach the terminal state with only one iteration. State 2 is connected to both 1 and 3, meaning that whatever transition is chosen, its shortest path to state 4 is by 2 transitions. Therefore, a simple optimal policy would be to go to state 4 whenever possible, or to any other one if not possible (state 2).\n",
        "\n",
        "Let's name the possible actions of our system:\n",
        "- 2 to 1: $a$\n",
        "- 2 to 3: $b$\n",
        "- 1 to 2: $c$\n",
        "- 1 to 4: $d$\n",
        "- 3 to 1: $e$\n",
        "- 3 to 4: $f$\n",
        "\n",
        "Then, here is a mapping of optimal action per state:\n",
        "\n",
        "|state|1|2|3|4|\n",
        "| -------- |||||\n",
        "|optimal action|$d$|$a$ or $b$|$f$|X|\n",
        "\n",
        "1) B: How would this change if from state 2, when taking action $a$ (moving from 2 to 1) we had a probability of 0.5 of remaining in state 2 and when taking action $b$ (moving from 2 to 3) we had a probability of 0.3 of remaining in state 2?\n",
        "\n",
        "Answer: If we were to apply such a modification to the system, a part of our policy should be reviewed. Optimal actions from states 1 and 3 do not change (they can still reach terminal state in 1 transition). However, we need to adress a change in the optimal action from state 2.\n",
        "For both actions $a$ and $b$, we now have a probabity in staying in state 2. In the case we stay in state 2, we would still be unable to reach terminal state and therefore would need to use at least another transition, reducing our reward again. Therefore, being action $a$ or $b$, we want to maximize our chances of not staying in state 2. Since, $a$ has a 50% percent chance of keeping us in state 2 and $b$ a 30% chance, if we were to study a large enough number of scenarios, the choice of state $b$ would result in more positive scores as it has a lower probability of reducing our reward. Therefore, with those modifications, action $b$ becomes more optimized over a.\n",
        "\n",
        "Then, here is an updated mapping of optimal action per state:\n",
        "\n",
        "|state|1|2|3|4|\n",
        "| -------- |||||\n",
        "|optimal action|$d$|$b$|$f$|X|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBJ7lYREx7PE"
      },
      "source": [
        "2) If now our policy $\\pi$ takes us from state 1 to 2, from 2 to 3, and from 3 to 4, what is $V_{\\pi}$?\n",
        "\n",
        "Answer: In case of a policy $\\pi$ defined as:\n",
        "\n",
        "|state|1|2|3|4|\n",
        "| -------- |||||\n",
        "|next state|2|3|4|X|\n",
        "\n",
        "We can use the Bellman equation to compute the $V_\\pi$ values for every state.\n",
        "Considering the Bellamn equation: $V_\\pi = \\Sigma_a \\pi(s\\|a) \\Sigma_{s'}T^a_{ss'}[R^a_{ss'}+\\gamma V^R(s')$, our values are:\n",
        "\n",
        "$V_\\pi(4)=0$ (terminal state)\n",
        "\n",
        "Now let's look at $V_\\pi(3)$ term by term:\n",
        "1. ($\\Sigma_a \\pi(s\\|a)...$):\n",
        "The value is a sum of expressions over all the possible actions, weighted by the probability of taking them under our policy ($\\pi(s\\|a)$). With the given policy, it is very simple as all our different states only have 1 possible action. Therefore, we will only have one element of weight 1 in our global sum.\n",
        "2. $...\\Sigma_{s'}T^a_{ss'}...$:\n",
        "Now, our single expression also contains a sum over all the possible states after taking the current action. The elements of the sum are weighted by their transition probability ($T^a_{ss'}$)  Once again, our only action takes us to a single possible state under policy $\\pi$. Therefore, we will have one element of weight 1 in our global sum.\n",
        "3. $...[R^a_{ss'}+\\gamma V_\\pi^R(s')]$:\n",
        "This last part of the equation sums the reward of taking action $a$ and transitioning from $s$ to $s'$ ($R^a_{ss'}$) and the $V_\\pi$ value of state $s'$ weighted by a discount factor. Considering all our transitions have a reward value of -1 and that our discount factor is 1, the value of $V\\pi(3)$ is:\n",
        "\n",
        "$V_\\pi(3)=1 * (-1+V^R(4))=-1$\n",
        "\n",
        "Propagating this logic to the other values, we get:\n",
        "\n",
        "$V_\\pi(2)=1 * (-1+V^R(3))=-2$  \n",
        "$V_\\pi(1)=1 * (-1+V^R(2))=-3$\n",
        "\n",
        "In conclusion, here are the values of $V_\\pi$:\n",
        "\n",
        "\n",
        "|state $s$|1|2|3|4|\n",
        "| -------- |||||\n",
        "|$V_\\pi (s)$|-3|-2|-1|0|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tJPwQJQx7PE"
      },
      "source": [
        "3) Perform one step of synchronous value iteration, considering that $V_1(s)=2, \\forall s \\in S-{4}$, $V_1(4)=0$ (terminal state), $\\gamma = 0.99$, and for each state we have uniform probability over the action space.\n",
        "\n",
        "Answer: By using the Bellman value update equation, we can perform an iteration with the given values. We start with:\n",
        "\n",
        "|s|1|2|3|4|\n",
        "|-----|||||\n",
        "|$V_1(s)$|2|2|2|0|\n",
        "\n",
        "We will perform the iteration on all states by following the Belllman update rule:\n",
        "$V_{i+1}(s)=max_{a\\in A}(\\Sigma_{s' \\in S}P(s' \\| s, a)[R(s, a, s')+ \\gamma V_i(s')])$\n",
        "\n",
        "In our case, we have uniform probability over the action space and for every action, only one possible resulting state. This will make the iteration very simple:\n",
        "\n",
        "$V_2(1)=max(1*[R(1, c, 2)+ 0.99*V_1(2)], 1*[R(1, d, 4)+ 0.99*V_1(4)])\\\\\n",
        "V_2(1)=max(1*[-1+0.99*2], 1*[-1+0.99*0])\\\\\n",
        "V_2(1)=max(0.98, -1)=0.98$\n",
        "\n",
        "$V_2(2)=max(1*[R(2, a, 1)+ 0.99*V_1(1)], 1*[R(2, b, 3)+ 0.99*V_1(3)])\\\\\n",
        "V_2(2)=max(1*[-1+0.99*2], 1*[-1+0.99*2])\\\\\n",
        "V_2(2)=max(0.98, 0.98)=0.98$\n",
        "\n",
        "$V_2(3)=max(1*[R(3, e, 1)+ 0.99*V_1(1)], 1*[R(3, f, 4)+ 0.99*V_1(4)])\\\\\n",
        "V_2(3)=max(1*[-1+0.99*2], 1*[-1+0.99*0])\\\\\n",
        "V_2(3)=max(0.98, -1)=0.98$\n",
        "\n",
        "In conclusion, after one iteration, we have:\n",
        "\n",
        "|s|1|2|3|4|\n",
        "|-----|||||\n",
        "|$V_2(s)$|0.98|0.98|0.98|0|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uC5pRPjx7PE"
      },
      "source": [
        "4) How do different values of gamma impact RL?\n",
        "\n",
        "Answer: The discount factor $\\gamma$ in Reinforcement Learning determines the balance of weights between immediate current rewards and future rewards. Its value is in between 0 and 1 and weighs $V$ in the Bellman equation inner sum.\n",
        "\n",
        "By choosing a high value of $\\gamma$, we will choose to keep a high portion of $V$'s value. This means that $V$ will have a stronger impact over the sum and thus, over the result value of the Bellman equation. Over multiple iterations, it means giving more weight to the future values of $V$ which are the values of long-term rewards.\n",
        "\n",
        "By choosing a low value of $\\gamma$, we will have only a small portion of $V$'s value. Then, its weigh will have a lesser impact on the Bellman equation's result, meaning the other term of the sum will determine the result. This other term is the value of the immediate reward of taking the chosen action in our context. In a nutshell, it means giving more weight to the immediate reward of taking this action compared to the value of $V$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_22_zRSx7PE"
      },
      "source": [
        "5) The agent starts in state 1 and selects the action to move to state 2. The reward for this transition is -1, and the Q-values are:  $Q(1,2) = -1.5$  and  $Q(1,4) = -0.5$. After arriving at state 2, the agent observes the Q-values:  $Q(2,1) = -2.0$  and  $Q(2,3) = -1.0$. Using a learning rate  $\\alpha = 0.1$  and  $\\gamma = 0.9$ , calculate the updated value of  $Q(1,2)$.\n",
        "\n",
        "Answer: To update the value of $Q(1, 2)$, we can use the Widrow-Hoff update rule. In our case, our actions only have one possible resulting state, so we will write this resulting state instead of the action name.  \n",
        "\n",
        "Rule: $Q^{t+1}(s, s')=Q^t(s, s') + \\alpha * ([r_t + \\gamma max_{s''}(Q^t(s', s'')] - Q^t(s, s'))$\n",
        "\n",
        "Following this rule, we have:\n",
        "\n",
        "$Q^2(1,2)=Q^1(1,2)+0.1*([-1+0.9*max(Q^t(2,1), Q^t(2,3))]-Q^1(1,2))\\\\\n",
        "Q^2(1,2)=-1.5+0.1*([-1+0.9*(-2.0)]+1.5)\\\\\n",
        "Q^2(1,2)=-1.63$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh-aWI5rx7PE"
      },
      "source": [
        "6) Describe the trade-off between exploration and exploitation in RL. How does epsilon-greedy attempt to tackle it? Under epsilon-greedy what is the probability of selecting the maximising action?\n",
        "\n",
        "Answer: Exploration describe the fact of trying actions whose consequence are not yet fully familiar for the system. Exploitation is about sticking to the information we currently have and pick the best known option.\n",
        "\n",
        "When we prioritize exploration, we make sure the long-term result is optimized as we will end up trying a much larger spectrum of possibilities, and then we will be able to come up with a better solution based on this experience. However, this strategy implies a long phase of experimentation that might be long.\n",
        "\n",
        "On the other hand, exploitation maximizes the immediate reward by instantly picking best actions based on the limited initial knowledge. This way, we can come up with decent solution faster but we might end up missing out on potential unknown better solutions.\n",
        "\n",
        "Both of these concepts are essential but it is very important to weigh them correctly because exploring too much might make us loose time by trying less efficient options and exploiting too much might trap us in a suboptimal strategy.\n",
        "\n",
        "The epsilon-greedy approach allows the system to give a chance to both strategies based on a set probability. Its concept is to give the system a probability $\\epsilon$ to choose a random action, ensuring the exploration of the unknown. Then, the system aso has a $1- \\epsilon$ chance of choosing to exploit the best known action yet. This way, we give room to both exploration and exploitation, and we can manually set the weight of each concept by wisely choosing $\\epsilon$.\n",
        "\n",
        "With the epsilon-greedy strategy, the pobability of selecting the maximizing action is the sum of the probability of selecting it initially and the probability of selecting it randomly.\n",
        "\n",
        "Probability of initially selecting it: $1-\\epsilon$\n",
        "\n",
        "Probability of selecting it randomly: $\\epsilon * \\frac{1}{|A|}$ (with A the set of possible actions)\n",
        "\n",
        "Therefore, the total probability of selecting the maximizing action is: $P=1-\\epsilon + \\frac{\\epsilon}{|A|}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_mo8pLXx7PE"
      },
      "source": [
        "## Part 2 (4 points)\n",
        "\n",
        "Now on to some RL implementation! This first exercise is in a custom environment which we will revisit again in Assignment 3. In this environment, your agent is a sous-chef tasked to prepare different kinds of pancakes.\n",
        "At first, we will keep it simple: the only ingredients your agent can play around with are pancake bread, bacon, and sauce, referred to with indices 0,1,2 respectively.\n",
        "\n",
        "The environment is given a set of recipes at the start, which will be the different goals the agent will randomly receive and try to reconstruct. Each goal is communicated to the agent at the start of an episode as a symbol (an index), which the agent does not have prior knowledge about: imagine that the head chef and your agent do not speak the same language, and that the head chef has left some different sketches for each recipe. Your agent will learn their meaning by interacting with the environment, preparing different pancakes and getting rewarded positively for getting the recipes right and negatively for getting them wrong.\n",
        "\n",
        "The observation your agent receives is the current pancake they have produced (e.g. [0, 1, 1] is a slice of pancake bread followed by two bacon strips), plus the symbol that represents one of the goals (e.g. 3 for goal 3, which the agent does not know anything about apriori).\n",
        "\n",
        "The actions your agent has at their disposal is to either position a pancake (0) a bacon strip (1) or some sauce (2).\n",
        "\n",
        "The episode terminates when the maximum size of the pancake is reached.\n",
        "\n",
        "At the start, the agent is then given a reward of +1 if it gets it right, -1 if it gets it wrong. This will change later!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CS1li75x7PE"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o21krVSCx7PF"
      },
      "outputs": [],
      "source": [
        "recipes = []\n",
        "recipes.append([0,1,2,0,1,2])\n",
        "recipes.append([0,1,1,0,1,1])\n",
        "recipes.append([0,1,1,0,2])\n",
        "recipes.append([0,2,0,2,0,2])\n",
        "recipes.append([0,1,1,1,1,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_92kJdZhx7PF"
      },
      "source": [
        "At the start, we will keep these goals fixed. Later we will ask you to try with different ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJBaQCPix7PF"
      },
      "outputs": [],
      "source": [
        "\"\"\"This is the environment for the pancake problem.\n",
        "The goal is to create a pancake with the same ingredients as the intended goal.\"\"\"\n",
        "\n",
        "class IngredientsActionSpace:\n",
        "    def __init__(self, n):\n",
        "        self.n = n  # Number of possible ingredients\n",
        "\n",
        "    def sample(self): #sample an action\n",
        "        return random.randint(0, self.n-1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "class PancakeEnv:\n",
        "    def __init__(self, goals, max_pancake_size=6, partial_reward=False):\n",
        "        #initialise the environment\n",
        "        self.goals = goals #list of goals\n",
        "        self.max_pancake_size = max_pancake_size #maximum number of ingredients in a pancake (including the pancake itself)\n",
        "        self.action_space = IngredientsActionSpace(3) #0 is the pancake, 1 is the bacon, 2 is the sauce\n",
        "        self.observation_space() #create the observation space\n",
        "        self.partial_reward = partial_reward #if we want to give a partial reward\n",
        "\n",
        "\n",
        "    def reset(self, goal = None): #reset the environment, choose a random goal if none is given\n",
        "        self.word_for_goal = random.randint(0, len(self.goals)-1) if goal is None else goal #choose a random goal (note that we pass an index, not the actual goal pancake!)\n",
        "        self.intended_goal = self.goals[self.word_for_goal] #get the actual goal pancake (we won't give it to the agent, but use it for computing the reward)\n",
        "        self.current_pancake = [0] #we always start with a pancake with no ingredients\n",
        "        return self.word_for_goal, self.current_pancake\n",
        "\n",
        "    def step(self, action): #act 1 step in the environment\n",
        "        self.current_pancake.append(action) #add the ingredient to the pancake\n",
        "        reward, done = self.compute_reward_and_done() #compute the reward and if the episode is done\n",
        "        return self.current_pancake, reward, done\n",
        "\n",
        "    def compute_reward_and_done(self):\n",
        "        if self.current_pancake == self.intended_goal: #we have reached the goal\n",
        "                return 1, True\n",
        "        if len(self.current_pancake) == self.max_pancake_size: # we have reached the maximum number of steps\n",
        "            #if we want to give a partial reward, we compute it here, otherwise we return 0 (failure)\n",
        "            return 0 if not self.partial_reward else partial_reward(self.intended_goal, self.current_pancake), True\n",
        "        return 0, False #we are not done yet\n",
        "\n",
        "\n",
        "    def observation_space(self):\n",
        "        # We have all the permutations of pancakes ingredients, from a length of 1 to max_pancake_size. list them all\n",
        "        # e.g. for max_pancake_size=3, we have\n",
        "        # [0] [0,0] [0,1] [0,2] [0,0,0] [0,0,1] [0,0,2], [0,1,0]...\n",
        "        self.observation_space = []\n",
        "        for i in range(1, self.max_pancake_size+1):\n",
        "            self.observation_space += self.permutations(i)\n",
        "\n",
        "    def permutations(self, n): #recursive function to get all the permutations of pancakes ingredients\n",
        "        if n == 1:\n",
        "            return [[0]]\n",
        "        perms = []\n",
        "        for perm in self.permutations(n-1):\n",
        "            for i in range(3):\n",
        "                perms.append(perm + [i])\n",
        "        return perms\n",
        "\n",
        "    def get_observation_index(self, current_pancake): #get the index of the current pancake in the observation space (used for indexing the Q-table)\n",
        "        return self.observation_space.index(current_pancake)\n",
        "\n",
        "def partial_reward(goal, current_pancake):\n",
        "    # we give a partial reward based on the number of ingredients that at the right place\n",
        "    reward = 0\n",
        "    for i in range(len(goal)):\n",
        "        if goal[i] == current_pancake[i]:\n",
        "            reward += 1\n",
        "    return reward/len(goal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-waNMWzcx7PF"
      },
      "source": [
        "At first, we start without partial reward, so our agent will be rewarded ONLY if the agent is able to reconstruct the whole pancake. This makes this simple reward actually very hard to learn, as we will see shortly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK5gpLcFx7PF",
        "outputId": "a924ca28-f2eb-42d1-f5e7-bfb3b5ba837e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, [0])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = PancakeEnv(recipes, max_pancake_size=6, partial_reward=False)\n",
        "env.reset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZbEgMYGx7PG"
      },
      "source": [
        "How large is the observation space? And the goal space? What about the action space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYUPT2Avx7PG"
      },
      "outputs": [],
      "source": [
        "print(\"Observation space size: \", len(env. ...))\n",
        "print(\"Goal space size: \", len(env. ...))\n",
        "print(\"Action space size: \", len(env. ...))\n",
        "print(\"Sample observation: \", env. ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8tj7KLex7PG"
      },
      "source": [
        "Try to sample a starting goal and observation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbRtgysqx7PG"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DieTel2fx7PG"
      },
      "source": [
        "# Q-Learning\n",
        "\n",
        "Q-Learning is a simple RL model-free algorithm which you have covered in the lecture.\n",
        "\n",
        "We have included the algorithm below. Can you implement it? We have implemented some helper functions but you should fill in the rest.\n",
        "\n",
        "<!-- ![text](algorithm.png) -->\n",
        "[![Q-learning algorithm](https://miro.medium.com/v2/resize:fit:1400/1*7AWfjw8YDfoRqnIO71DjiA.png)](https://en.wikipedia.org/wiki/Q-learning)\n",
        "\n",
        "Note that in this case, actions will have different consequences depending on the goal we are aiming to achieve. This means that we cannot have one single Q-table for each goal, or our updates will be conflicting. The first dimension of the Q-table should be the number of goals!\n",
        "\n",
        "In reality, the goal symbol is actually part of the state space. We keep it separate from the observation space though as it will be easier to index both with what the environment returns us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uImYuCY4x7PG"
      },
      "outputs": [],
      "source": [
        "def q_table(): #initialise the Q-table\n",
        "  Qtable = np.zeros(...)\n",
        "  return Qtable\n",
        "\n",
        "def greedy_policy(Qtable, ...): #choose the action with the highest Q-value\n",
        "  action = ...\n",
        "  return int(action)\n",
        "\n",
        "def epsilon_greedy_policy(Qtable, ...): #choose a random action with probability epsilon, otherwise choose the greedy action\n",
        "    ...\n",
        "\n",
        "def random_policy():\n",
        "    return env.action_space.sample()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlEEC_CRx7PG"
      },
      "source": [
        "Here we provide some parameters for you. Feel free to play around with them and see how good your agent gets!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy4kUj43x7PG"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 100000  # Total training episodes\n",
        "max_steps = 5 # DO NOT CHANGE: Max steps per episode (ingredients you can place)\n",
        "learning_rate = 1          # Learning rate\n",
        "\n",
        "# Environment parameters\n",
        "gamma = 0.99                # Discounting rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8q5Jozhx7PH"
      },
      "source": [
        "Try to experiment with different $\\epsilon$ , and see how much exploration is required to solve this problem.\n",
        "A good idea is always to also try a decaying epsilon when fixed epsilons are not doing great, granting more exploration at the start ($\\epsilon$ close to 1), and more exploitation towards the end ($\\epsilon$ close to 0).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAW4S59qx7PH"
      },
      "outputs": [],
      "source": [
        "\"Optional: implement a function to decay epsilon over time and plot it\"\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG2q0BNix7PH"
      },
      "outputs": [],
      "source": [
        "rewards = []\n",
        "Qtable = q_table(...)\n",
        "for episode in range(n_training_episodes):\n",
        "    goal, state = env.reset()\n",
        "    done = False\n",
        "    for step_n in range(max_steps):\n",
        "        state_idx = env.get_observation_index(state)\n",
        "        action = ...\n",
        "        new_state, reward, done = env.step(action)\n",
        "        new_state_idx = env.get_observation_index(new_state)\n",
        "        Qtable[...] = ...\n",
        "        state = new_state\n",
        "        if done:\n",
        "            print(f\"Episode {episode}, reward {reward}\")\n",
        "            rewards.append(reward)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFqcwdi6x7PH"
      },
      "outputs": [],
      "source": [
        "\"Plot the rewards - depending on your exploration strategy, the curve might be hard to interpret,\"\n",
        "\"try using a moving average to smooth it out\"\n",
        "\n",
        "def moving_average(x, window_size=None):\n",
        "    if window_size is None:\n",
        "        window_size = len(x)//10\n",
        "    return np.convolve(x, np.ones(window_size), 'valid') / window_size\n",
        "\n",
        "#plt.plot(moving_average(rewards, 1000))\n",
        "plt.plot(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQA83tI3x7PH"
      },
      "source": [
        "Do not worry if you don't manage to get an agent that has perfectly learnt the environment, try to obtain the best reward plot you can get.\n",
        "\n",
        "We are going to try change the reward strategy of the environment, meaning that now our agent receives a partial reward at the end of each episode, i.e. the fraction of ingredients that match with the actual ingredient of the recipe.\n",
        "\n",
        "Try again to train a new Q-learning agent and obtain the rewards plot. Compare the previous plot and this one qualitatively!\n",
        "\n",
        "*(Optional)* For both environments (with and without partial reward) it can help to be aware of how a random agent would perform on the environment, just to have a very low baseline, to understand if our agent is learning anything. Feel free to use the random_policy() function and plot how the rewards look for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj41dbHGx7PH"
      },
      "outputs": [],
      "source": [
        "env = PancakeEnv(recipes, max_pancake_size=6, partial_reward=True)\n",
        "\n",
        "\"\"\"Your code here\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWrEFxHyx7PH"
      },
      "source": [
        "\n",
        "# Part 3: Blackjack! (3 points)\n",
        "\n",
        "For those that are unfamiliar, blackjack is a common card game. There are (at least) two-agents, the dealer and the player. In our case, at the beginning of the game the dealer gives our agent one card. The dealer's card is placed face up. The dealer also gets two cards: one is face up, the other face down.\n",
        "The player observes their own card and the dealer's card and must decide to 'hit' (i.e. get a new card) or to 'stick'(i.e. keep the hand that they've got).\n",
        "\n",
        "If the player selects 'stick' it is then the dealers turn, which will turn the hidden card face-up, and then hit until their sum is at least or they bust (they go over 21). The goal is to beat the dealer, i.e. get a higher sum than them without busting ourselves.\n",
        "\n",
        "This environment comes from the popular gymnasium library, which has a lot of different RL environments for you to try. You can read more info about this environment and the rules of blackjack it follows at https://gymnasium.farama.org/environments/toy_text/blackjack/.\n",
        "\n",
        "Here are the main points:\n",
        "\n",
        "**Action Space**\n",
        "\n",
        "There are two actions: stick (0), and hit (1).\n",
        "\n",
        "**Observation Space**\n",
        "\n",
        "The observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1).\n",
        "\n",
        "**Rewards**\n",
        "\n",
        "win game: +1\n",
        "\n",
        "lose game: -1\n",
        "\n",
        "draw game: 0\n",
        "\n",
        "\n",
        "[![Blackjack!](https://www.gymlibrary.dev/_images/blackjack.gif)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiJxnBxFx7PH"
      },
      "outputs": [],
      "source": [
        "# !pip install gym #uncomment if you haven't installed gym yet\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9cQOrCFx7PH"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Blackjack-v1\", sab=True)\n",
        "\n",
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space\n",
        "print(f\"Number of states: {state_size}, Number of actions: {action_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDewlo97x7PI"
      },
      "outputs": [],
      "source": [
        "# again, hyperparameters that you can change\n",
        "learning_rate = 0.01\n",
        "n_training_episodes = 100000 # you can try also 1000000, it might take a while\n",
        "epsilon = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "394VV1hHx7PI"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "print(f\"Initial state: {state}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFEilA_tx7PI"
      },
      "source": [
        "Note above how the observation looks. As we said before, the states are tuples of 3 values. Modify the functions defining the Q-table and your policies accordingly (if needed), keeping in mind we will need to store 2 q-values for each combination of these values, since we have two actions.\n",
        "\n",
        "P.S. Note that you can reuse a lot of code from Part 2, and you are also encouraged to!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tr2zZgHx7PI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def q_table(): #initialise the Q-table\n",
        "  Qtable = ...\n",
        "  return Qtable\n",
        "\n",
        "def greedy_policy(...): #choose the action with the highest Q-value\n",
        "  action = ...\n",
        "  return int(action)\n",
        "\n",
        "def epsilon_greedy_policy(...): #choose a random action with probability epsilon, otherwise choose the greedy action\n",
        "    ...\n",
        "\n",
        "def random_policy():\n",
        "    return env.action_space.sample()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taGKYpWNx7PI"
      },
      "outputs": [],
      "source": [
        "rewards = []\n",
        "for episode in range(n_training_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    cur_reward = 0\n",
        "    while not done:\n",
        "        action = ...\n",
        "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        cur_reward += ...\n",
        "        Qtable[...] = ...\n",
        "        state = new_state\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            rewards.append(cur_reward)\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay_iLEI7x7PI"
      },
      "source": [
        "Let's plot the rewards and see if the agent is learning something. Again, it might be a good idea to plot a moving average of your rewards, as depending on your exploration tactic the reward might be very noisy.\n",
        "\n",
        "What result do you get? Does it seem reasonable given our expectations knowing the blackjack game (the dealer always have the advantage)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlhiT3xtx7PI"
      },
      "outputs": [],
      "source": [
        "# Plot the rewards\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYaykJ74x7PI"
      },
      "source": [
        "Let's try to see how good our agent is! Try to see what the average reward your agent gets in 100 episodes.\n",
        "(What is the difference from the training loop above? Since we are now testing, do we want to update the Qtable? Also, what is the policy we want to use now?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAwjmrWqx7PI"
      },
      "outputs": [],
      "source": [
        "# Test the agent, get the average reward\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpVYZwa6x7PI"
      },
      "source": [
        "## Visualize the agent's policy\n",
        "\n",
        "Note that this code works with the Q-table being represented as a dictionary with keys being the observations and values being numpy arrays of length 2 (one element for each action). If you have represented your Q-table as an n-dimensional array, you might have to modify the create_grids function (or change the Q-table representation to a dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD_Gb2pdx7PI"
      },
      "outputs": [],
      "source": [
        "#!pip install seaborn #uncomment if you haven't installed seaborn yet\n",
        "\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "def create_grids(Qtable, usable_ace=False):\n",
        "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
        "    # convert our state-action values to state values\n",
        "    # and build a policy dictionary that maps observations to actions\n",
        "    state_value = defaultdict(float)\n",
        "    policy = defaultdict(int)\n",
        "    # If Qtable is an n-dimensional numpy array, we need to convert it to a dictionary\n",
        "    if isinstance(Qtable, np.ndarray): #changes the Qtable to a dictionary if it is a numpy array\n",
        "        Qtable = {k: v for k, v in np.ndenumerate(Qtable)}\n",
        "    for obs, action_values in Qtable.items():\n",
        "        state_value[obs] = float(np.max(action_values))\n",
        "        policy[obs] = int(np.argmax(action_values))\n",
        "\n",
        "    player_count, dealer_count = np.meshgrid(\n",
        "        # players count, dealers face-up card\n",
        "        np.arange(12, 22),\n",
        "        np.arange(1, 11),\n",
        "    )\n",
        "\n",
        "    # create the value grid for plotting\n",
        "    value = np.apply_along_axis(\n",
        "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
        "        axis=2,\n",
        "        arr=np.dstack([player_count, dealer_count]),\n",
        "    )\n",
        "    value_grid = player_count, dealer_count, value\n",
        "\n",
        "    # create the policy grid for plotting\n",
        "    policy_grid = np.apply_along_axis(\n",
        "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
        "        axis=2,\n",
        "        arr=np.dstack([player_count, dealer_count]),\n",
        "    )\n",
        "    return value_grid, policy_grid\n",
        "\n",
        "\n",
        "def create_plots(value_grid, policy_grid, title: str):\n",
        "    #Creates a plot using a value and policy grid.\n",
        "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
        "    player_count, dealer_count, value = value_grid\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # plot the state values\n",
        "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
        "    ax1.plot_surface(\n",
        "        player_count,\n",
        "        dealer_count,\n",
        "        value,\n",
        "        rstride=1,\n",
        "        cstride=1,\n",
        "        cmap=\"viridis\",\n",
        "        edgecolor=\"none\",\n",
        "    )\n",
        "    plt.xticks(range(12, 22), range(12, 22))\n",
        "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
        "    ax1.set_title(f\"State values: {title}\")\n",
        "    ax1.set_xlabel(\"Player sum\")\n",
        "    ax1.set_ylabel(\"Dealer showing\")\n",
        "    ax1.zaxis.set_rotate_label(False)\n",
        "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
        "    ax1.view_init(20, 220)\n",
        "\n",
        "    # plot the policy\n",
        "    fig.add_subplot(1, 2, 2)\n",
        "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
        "    ax2.set_title(f\"Policy: {title}\")\n",
        "    ax2.set_xlabel(\"Player sum\")\n",
        "    ax2.set_ylabel(\"Dealer showing\")\n",
        "    ax2.set_xticklabels(range(12, 22))\n",
        "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
        "\n",
        "    # add a legend\n",
        "    legend_elements = [\n",
        "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
        "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
        "    ]\n",
        "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
        "    return fig\n",
        "\n",
        "\n",
        "# state values & policy with usable ace (ace counts as 11)\n",
        "value_grid, policy_grid = create_grids(Qtable, usable_ace=True)\n",
        "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvfkk9-3x7PJ"
      },
      "outputs": [],
      "source": [
        "# state values & policy without usable ace (ace counts as 1)\n",
        "value_grid, policy_grid = create_grids(Qtable, usable_ace=False)\n",
        "fig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RAjZhd-x7PJ"
      },
      "source": [
        "Can you see some patterns in the value function and policy plots of your agents? What is the difference between when we have a usable ace or not at the start? Comment below:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}